version: "3.9"

x-airflow-common: &airflow-common
  build:
    context: ./airflow
  environment:
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__LOAD_EXAMPLES: "False"
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "False"
    AIRFLOW__CORE__DEFAULT_TIMEZONE: Asia/Seoul
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: "True"
    KAFKA_BOOTSTRAP_SERVERS: kafka:9092
    NAVER_CLIENT_ID: ${NAVER_CLIENT_ID}
    NAVER_CLIENT_SECRET: ${NAVER_CLIENT_SECRET}
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow:/opt/airflow/airflow
    - ./kafka:/opt/airflow/kafka  # producer 모듈 import 경로
    - airflow_logs:/opt/airflow/logs
    - airflow_plugins:/opt/airflow/plugins
  dns:
    - 8.8.8.8
    - 8.8.4.4
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    postgres:
      condition: service_healthy
    kafka:
      condition: service_healthy

x-spark-image: &spark_image
  build:
    context: .
    dockerfile: ./spark/Dockerfile
  image: trend_spark_custom:latest

services:
  # ──────────────────────────────────────────────
  # Kafka 스택
  # ──────────────────────────────────────────────
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.0
    hostname: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.6.0
    hostname: kafka
    depends_on:
      - zookeeper
    ports:
      - "29092:29092"   # 로컬 디버깅용 (host → localhost:29092)
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9092 2>/dev/null | head -1"]
      interval: 10s
      timeout: 10s
      retries: 15
      start_period: 30s

  kafka-init:
    image: confluentinc/cp-kafka:7.6.0
    depends_on:
      kafka:
        condition: service_healthy
    volumes:
      - ./kafka/init-topics.sh:/init-topics.sh
    entrypoint: ["/bin/bash", "/init-topics.sh"]
    restart: "no"

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    ports:
      - "8085:8080"   # 호스트 8085 → 컨테이너 8080
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
    depends_on:
      kafka:
        condition: service_healthy

  blog-consumer:
    build:
      context: ./kafka
      dockerfile: Dockerfile.consumer
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      KAFKA_TOPIC: blog.raw
      KAFKA_GROUP_ID: blog-raw-mysql-sink
      KAFKA_AUTO_OFFSET_RESET: earliest
      MYSQL_HOST: ${CONSUMER_MYSQL_HOST:-host.docker.internal}
      MYSQL_PORT: ${CONSUMER_MYSQL_PORT:-3306}
      MYSQL_DATABASE: ${CONSUMER_MYSQL_DATABASE:-blog_db}
      MYSQL_USER: ${CONSUMER_MYSQL_USER}
      MYSQL_PASSWORD: ${CONSUMER_MYSQL_PASSWORD}
    depends_on:
      kafka:
        condition: service_healthy
    restart: unless-stopped

  # ──────────────────────────────────────────────
  # Airflow 스택
  # ──────────────────────────────────────────────
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow_postgres:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 5s
      timeout: 5s
      retries: 10

  airflow-init:
    <<: *airflow-common
    command: >
      bash -lc "airflow db migrate &&
      airflow users create --username admin --password admin --firstname admin --lastname admin --role Admin --email admin@example.com || true"
    restart: "no"

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    restart: always

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    restart: always

  # ──────────────────────────────────────────────
  # Flink + Redis 스택
  # ──────────────────────────────────────────────
  redis:
    image: redis:7-alpine
    hostname: redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 10

  flink-jobmanager:
    build:
      context: ./flink
    hostname: flink-jobmanager
    command: jobmanager
    ports:
      - "8081:8081"
    environment:
      JOB_MANAGER_RPC_ADDRESS: flink-jobmanager
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      KAFKA_TOPIC: blog.cleaned
      KAFKA_GROUP_ID: flink-traffic-10m
      KAFKA_STARTING_OFFSETS: latest
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_DB: 0
      REDIS_KEY: trend:traffic:10m
    volumes:
      - ./flink/traffic_10m_to_redis_job.py:/opt/flink/usrlib/traffic_10m_to_redis_job.py
    depends_on:
      kafka:
        condition: service_healthy
      redis:
        condition: service_healthy

  flink-taskmanager:
    build:
      context: ./flink
    hostname: flink-taskmanager
    command: taskmanager
    environment:
      JOB_MANAGER_RPC_ADDRESS: flink-jobmanager
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      KAFKA_TOPIC: blog.cleaned
      KAFKA_GROUP_ID: flink-traffic-10m
      KAFKA_STARTING_OFFSETS: latest
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_DB: 0
      REDIS_KEY: trend:traffic:10m
    volumes:
      - ./flink/traffic_10m_to_redis_job.py:/opt/flink/usrlib/traffic_10m_to_redis_job.py
    depends_on:
      flink-jobmanager:
        condition: service_started
      kafka:
        condition: service_healthy
      redis:
        condition: service_healthy

  # ──────────────────────────────────────────────
  # Spark (Batch Prediction)
  # ──────────────────────────────────────────────
  spark-master:
    <<: *spark_image
    hostname: spark-master
    environment:
      - TZ=Asia/Seoul
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - PYTHONPATH=/opt/spark-app/spark/daily_pipeline:/opt/spark-app/spark
    volumes:
      - ./spark:/opt/spark-app/spark
    ports:
      - "7077:7077"
      - "8088:8080"  # airflow(8080) 충돌 방지

  spark-worker-1:
    <<: *spark_image
    hostname: spark-worker-1
    environment:
      - TZ=Asia/Seoul
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - PYTHONPATH=/opt/spark-app/spark
    volumes:
      - ./spark:/opt/spark-app/spark
    depends_on:
      - spark-master

  spark-batch:
    <<: *spark_image
    hostname: spark-batch
    depends_on:
      mysql:
        condition: service_healthy
      spark-master:
        condition: service_started
    environment:
      TZ: Asia/Seoul
      PYTHONPATH: /opt/spark-app/spark

      # MySQL (compose 내부 mysql 서비스 사용)
      MYSQL_HOST: mysql
      MYSQL_PORT: 3306
      MYSQL_DATABASE: blog_db
      MYSQL_USER: root
      MYSQL_PASSWORD: root

      # Table names
      RAW_TABLE: "${RAW_TABLE:-blog_post}"
      CLEANED_TABLE: "${CLEANED_TABLE:-cleaned_docs}"
      STATS_TABLE: "${STATS_TABLE:-daily_token_stats}"
      PRED_TABLE: "${PRED_TABLE:-predictions}"

      # discovered_at timezone handling
      DISCOVERED_AT_IS_UTC: "${DISCOVERED_AT_IS_UTC:-true}"

      # Pipeline
      RUN_DATE: "${RUN_DATE:-2026-02-25}"
      WINDOW_DAYS: "${WINDOW_DAYS:-7}"
      TOP_N: "${TOP_N:-50}"
      REBUILD_WINDOW: "${REBUILD_WINDOW:-true}"

      # Candidate thresholds
      MIN_DOC_TODAY: "${MIN_DOC_TODAY:-5}"
      MIN_COUNT_TODAY: "${MIN_COUNT_TODAY:-10}"

      # Burst thresholds
      GROWTH_RATIO_MIN: "${GROWTH_RATIO_MIN:-2.0}"
      PREV_MEAN_MAX: "${PREV_MEAN_MAX:-3.0}"

      # Score weights
      W_BURST: "${W_BURST:-1.5}"
      W_STABLE: "${W_STABLE:-0.7}"
      W_UNCERTAIN: "${W_UNCERTAIN:-0.3}"

      # Token filters
      MIN_TOKEN_LEN: "${MIN_TOKEN_LEN:-2}"
      MAX_TOKEN_LEN: "${MAX_TOKEN_LEN:-20}"

      # Tokenizer
      TOKENIZER_MODE: "${TOKENIZER_MODE:-morph}"
      MORPH_ALLOWED_TAGS: "${MORPH_ALLOWED_TAGS:-NNG,NNP,NNB,NR,NP,SL}"

      # Stopwords (지니 현재 spark 폴더 구조 기준)
      STOPWORDS_PATH: "${STOPWORDS_PATH:-/opt/spark-app/spark/resources/stopwords_ko.txt}"

    volumes:
      - ./spark:/opt/spark-app/spark

    command: >
      bash -lc "
      spark-submit \
        --master spark://spark-master:7077 \
        --conf spark.sql.session.timeZone=Asia/Seoul \
        --conf spark.driverEnv.PYTHONPATH=/opt/spark-app/spark \
        --conf spark.executorEnv.PYTHONPATH=/opt/spark-app/spark \
        --conf spark.driver.extraClassPath=/opt/bitnami/spark/jars/mysql-connector-j-8.4.0.jar \
        --conf spark.executor.extraClassPath=/opt/bitnami/spark/jars/mysql-connector-j-8.4.0.jar \
        /opt/spark-app/spark/daily_pipeline/daily_pipeline_job.py \
          --run-date $${RUN_DATE} \
          --window-days $${WINDOW_DAYS} \
          --top-n $${TOP_N} \
          --rebuild-window $${REBUILD_WINDOW}
      "

volumes:
  airflow_postgres:
  airflow_logs:
  airflow_plugins:
  redis_data:
  mysql_data:
